{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# wandb.init(project=\"nlp_project_rtv\", name=\"bert base uncased test run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "DEV_CLAIMS_BASELINE_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/dev-claims-baseline.json\"\n",
    "DEV_CLAIMS_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/dev-claims.json\"\n",
    "EVIDENCE_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/evidence.json\"\n",
    "SMALL_EVIDENCE_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/small_evidence.json\"\n",
    "TINY_EVIDENCE_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/tiny_evidence.json\"\n",
    "CODE_DEV_EVIDENCE_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/code_dev_evidence.json\"\n",
    "TEST_CLAIMS_UNLABELLED_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/test-claims-unlabelled.json\"\n",
    "TRAIN_CLAIMS_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/train-claims.json\"\n",
    "\n",
    "# ARGS\n",
    "BATCH_SIZE = 4\n",
    "EPOCH = 1\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LR = 2e-5\n",
    "MAX_LENGTH = 64\n",
    "RETRIEVAL_NUM = 3\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mps\n"
     ]
    }
   ],
   "source": [
    "# bert model\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"using mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"using cpu\")\n",
    "\n",
    "model = model.to(device)  # Move model to device\n",
    "\n",
    "# Instantiate the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=MAX_LR, weight_decay=1e-4)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Instantiate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/rtv/model/rtv_13_05_2023/rtv_checkpoint.bin\"\n",
    "load_model = True\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(pretrained_model))\n",
    "# model.cuda() # for NVIDIA GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small util\n",
    "def makedir(sub_dir):\n",
    "    date = datetime.now().strftime(\"%d_%m_%Y\")\n",
    "    save_dir = f\"./model/{sub_dir}_{date}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    return save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small util\n",
    "def pre_process(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    cleaned = ''.join([char if char.isalnum() or char.isspace() else '' for char in sentence])\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small util\n",
    "def tokenization(text):\n",
    "    tokens = tokenizer(text, max_length=MAX_LENGTH, padding=True, return_tensors=\"pt\", truncation=True)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute embeddings for either claim or evidence\n",
    "def extract_embeddings(batch, prefix):\n",
    "    embeddings = model(input_ids=batch[f\"batched_{prefix}_input_ids\"],attention_mask=batch[f\"batched_{prefix}_attention_mask\"]).last_hidden_state\n",
    "    embeddings = embeddings[:, 0, :]\n",
    "    return torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "def extract_origin_embedding(claim_token):\n",
    "    claim_embedding = model(input_ids=claim_token.input_ids, attention_mask=claim_token.attention_mask).last_hidden_state\n",
    "    claim_embedding = claim_embedding[:, 0, :]\n",
    "    return torch.nn.functional.normalize(claim_embedding, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSet(Dataset):\n",
    "    def __init__(self):\n",
    "        with open(TEST_CLAIMS_UNLABELLED_JSON_PATH, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.all_claim_keys = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_claim_keys)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        claim_id = self.all_claim_keys[id]\n",
    "        claim = self.data.get(claim_id)\n",
    "        return [claim, claim_id, claim[\"claim_text\"]]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batched_encoding = dict()\n",
    "    \n",
    "        claims, claim_ids, claim_texts = zip(*[(claim, claim_id, pre_process(claim_text)) for claim, claim_id, claim_text in batch])\n",
    "        batched_encoding[\"batched_claims\"] = claims\n",
    "        batched_encoding[\"batched_claim_ids\"] = claim_ids\n",
    "        claim_tokens = tokenization(claim_texts)\n",
    "        batched_encoding[\"batched_claim_input_ids\"] = claim_tokens.input_ids\n",
    "        batched_encoding[\"batched_claim_attention_mask\"] = claim_tokens.attention_mask\n",
    "\n",
    "        return batched_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationSet(Dataset):\n",
    "    def __init__(self):\n",
    "        with open(DEV_CLAIMS_JSON_PATH, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.all_claim_keys = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_claim_keys)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        claim_id = self.all_claim_keys[id]\n",
    "        claim = self.data.get(claim_id)\n",
    "        claim_text = pre_process(claim[\"claim_text\"])\n",
    "        return [claim, claim_id, claim_text]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batched_encoding = dict()\n",
    "        \n",
    "        claims, claim_ids, claim_texts = zip(*[(claim, claim_id, claim_text) for claim, claim_id, claim_text in batch])\n",
    "        \n",
    "        batched_encoding[\"batched_claims\"] = claims\n",
    "        batched_encoding[\"batched_claim_ids\"] = claim_ids\n",
    "\n",
    "        claim_tokens = tokenization(claim_texts)\n",
    "        batched_encoding[\"batched_claim_input_ids\"] = claim_tokens.input_ids\n",
    "        batched_encoding[\"batched_claim_attention_mask\"] = claim_tokens.attention_mask\n",
    "        \n",
    "        evidences = [claim[\"evidences\"] for claim, claim_id, claim_text in batch]\n",
    "        batched_encoding[\"batched_claim_evidences\"] = evidences\n",
    "\n",
    "        return batched_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvidenceSet(Dataset):\n",
    "    def __init__(self):\n",
    "        with open(CODE_DEV_EVIDENCE_JSON_PATH, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.all_evidence_keys = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_evidence_keys)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        evidence_id = self.all_evidence_keys[id]\n",
    "        evidence = pre_process(self.data[evidence_id])\n",
    "        return [evidence_id, evidence]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batched_encoding = dict()\n",
    "        \n",
    "        evidence_ids, evidence_texts = zip(*batch)\n",
    "        batched_encoding[\"batched_evidence_ids\"] = evidence_ids\n",
    "\n",
    "        evidence_tokens = tokenization(evidence_texts)\n",
    "        batched_encoding[\"batched_evidence_input_ids\"] = evidence_tokens.input_ids\n",
    "        batched_encoding[\"batched_evidence_attention_mask\"] = evidence_tokens.attention_mask\n",
    "        \n",
    "        return batched_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSet(Dataset):\n",
    "    def __init__(self, evidence_embeddings, evidence_ids):\n",
    "        self.evidence_embeddings = evidence_embeddings.to(device)\n",
    "        self.evidence_ids = evidence_ids\n",
    "        self.data = self.in_batch_negative_samples()\n",
    "        self.all_claim_keys = list(self.data.keys())\n",
    "        with open(CODE_DEV_EVIDENCE_JSON_PATH, \"r\") as f:\n",
    "            self.evidences = json.load(f)\n",
    "        self.all_evidence_keys = list(self.evidences.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_claim_keys)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        claim_id = self.all_claim_keys[id]\n",
    "        claim = self.data.get(claim_id)\n",
    "        return claim \n",
    "\n",
    "    def in_batch_negative_samples(self, batch_size=1):\n",
    "        with open(DEV_CLAIMS_JSON_PATH, \"r\") as f:\n",
    "            train_set = json.load(f)\n",
    "        train_with_negative = dict()\n",
    "\n",
    "        # Iterate over batches of data\n",
    "        for i in range(0, len(train_set), batch_size):\n",
    "            batch_items = list(train_set.items())[i:i+batch_size]\n",
    "            claim_texts = [pre_process(item[1]['claim_text']) for item in batch_items]\n",
    "\n",
    "            # Tokenize the claim texts\n",
    "            claim_token = tokenization(claim_texts)\n",
    "            claim_token = claim_token.to(device)\n",
    "            claim_embedding = extract_origin_embedding(claim_token)\n",
    "\n",
    "            # Compute scores between claim embeddings and evidence embeddings\n",
    "            similarity = torch.mm(claim_embedding, self.evidence_embeddings.t())\n",
    "            similar_ids = torch.topk(similarity, k=K, dim=1).indices.tolist()\n",
    "\n",
    "            # Iterate over the items in the current batch\n",
    "            for idx, (claim_id, claim) in enumerate(batch_items):\n",
    "                negative_evidences = []\n",
    "                \n",
    "                for i in similar_ids[idx]:\n",
    "                    if self.evidence_ids[i] not in claim[\"evidences\"]:\n",
    "                        negative_evidences.append(self.evidence_ids[i])\n",
    "                \n",
    "                claim[\"negative_evidences\"] = negative_evidences\n",
    "                train_with_negative[claim_id] = claim\n",
    "            \n",
    "        return train_with_negative\n",
    "\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        batched_encoding = dict()\n",
    "\n",
    "        claims = [claim for claim in batch]\n",
    "\n",
    "        related_ids = [id for claim in batch for id in claim[\"evidences\"]]\n",
    "        unrelated_ids = [id for claim in batch for id in claim[\"negative_evidences\"]]\n",
    "\n",
    "        labels = [[1]*len(claim[\"evidences\"]) for claim in batch]\n",
    "\n",
    "        claim_texts = [pre_process(claim[\"claim_text\"]) for claim in claims]\n",
    "        claim_tokens = tokenization(claim_texts)\n",
    "\n",
    "        related_evidence_texts = [pre_process(self.evidences[id]) for id in related_ids]\n",
    "        unrelated_evidence_texts = [pre_process(self.evidences[id]) for id in unrelated_ids]\n",
    "        evidences_texts = related_evidence_texts + unrelated_evidence_texts\n",
    "\n",
    "        evidence_tokens = tokenization(evidences_texts)\n",
    "\n",
    "        batched_encoding[\"batched_claim_input_ids\"] = claim_tokens.input_ids\n",
    "        batched_encoding[\"batched_claim_attention_mask\"] = claim_tokens.attention_mask\n",
    "        batched_encoding[\"batched_evidence_input_ids\"] = evidence_tokens.input_ids\n",
    "        batched_encoding[\"batched_evidence_attention_mask\"] = evidence_tokens.attention_mask\n",
    "        batched_encoding[\"labels\"] = labels\n",
    "\n",
    "        return batched_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predication\n",
    "load_emb = False\n",
    "if load_emb == True:\n",
    "    best_evidence_embeddings = torch.load(\"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/rtv/model/rtv_13_05_2023/evidence_embeddings\")\n",
    "    best_evidence_ids = torch.load(\"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/rtv/model/rtv_13_05_2023/evidence_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_evidence(evi_dataloader, model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # place holder\n",
    "    total_samples = len(evi_dataloader.dataset)\n",
    "    embedding_dim = model.config.hidden_size\n",
    "    evidence_embeddings = torch.zeros(total_samples, embedding_dim).to(device)\n",
    "    evidence_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(evi_dataloader)):\n",
    "            batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "            \n",
    "            evidence_embedding = extract_embeddings(batch, \"evidence\")\n",
    "            start_index = i * evi_dataloader.batch_size\n",
    "            end_index = start_index + evidence_embedding.size(0)\n",
    "            evidence_embeddings[start_index:end_index] = evidence_embedding\n",
    "            evidence_ids.extend(batch[\"batched_evidence_ids\"])\n",
    "\n",
    "    return evidence_embeddings, evidence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_dataloader, evidence_embeddings, evidence_ids, model):\n",
    "    \n",
    "    f_scores = []\n",
    "    model.eval()\n",
    "    evidence_embeddings = evidence_embeddings.to(device).t()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "            batched_claim_embeddings = extract_embeddings(batch, \"claim\")\n",
    "\n",
    "            similarity = torch.mm(batched_claim_embeddings, evidence_embeddings)\n",
    "            retrieved_ids = torch.topk(similarity, k=RETRIEVAL_NUM, dim=1).indices.tolist()\n",
    "\n",
    "            for idx, data in enumerate(batch[\"batched_claims\"]):\n",
    "                evidence_correct = 0\n",
    "                pred_evidences = [evidence_ids[i] for i in retrieved_ids[idx]]\n",
    "                \n",
    "                for evidence_id in batch[\"batched_claim_evidences\"][idx]:\n",
    "                    if evidence_id in pred_evidences:\n",
    "                        evidence_correct += 1\n",
    "\n",
    "                if evidence_correct > 0:\n",
    "                    evidence_recall = float(evidence_correct) / len(batch[\"batched_claim_evidences\"][idx])\n",
    "                    evidence_precision = float(evidence_correct) / len(pred_evidences)\n",
    "                    evidence_fscore = (2 * evidence_precision * evidence_recall) / (evidence_precision + evidence_recall)\n",
    "                \n",
    "                else:\n",
    "                    evidence_fscore = 0\n",
    "\n",
    "                f_scores.append(evidence_fscore)\n",
    "\n",
    "    return sum(f_scores) / len(f_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(evidence_embeddings, evidence_ids):\n",
    "    test_set = TestSet()\n",
    "    dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, collate_fn=test_set.collate_fn)\n",
    "\n",
    "    predicted = {}\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "        claim_embedding = extract_embeddings(batch, \"claim\")\n",
    "        \n",
    "        similarity = torch.mm(claim_embedding, evidence_embeddings.t())\n",
    "        topk_ids = torch.topk(similarity, k=RETRIEVAL_NUM, dim=1).indices.tolist()\n",
    "\n",
    "        for idx, claim in enumerate(batch[\"batched_claims\"]):\n",
    "            claim[\"evidences\"] = [evidence_ids[i] for i in topk_ids[idx]]\n",
    "            predicted[batch[\"batched_claim_ids\"][idx]] = claim\n",
    "\n",
    "    with open(\"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/prediction/retrieval-test-claims.json\", 'w') as file:\n",
    "        json.dump(predicted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the loss\n",
    "def compute_loss(claim_embeddings, evidence_embeddings, batch):\n",
    "\n",
    "    # Compute cosine similarities and scores\n",
    "    similarity = torch.mm(claim_embeddings, evidence_embeddings.t())\n",
    "    negative_log_likelihood  = - torch.nn.functional.log_softmax(similarity / 0.1, dim=1)\n",
    "\n",
    "    loss = []\n",
    "    s = 0\n",
    "    # Iterate over each label in the batch\n",
    "    for idx, labels in enumerate(batch[\"labels\"]):\n",
    "        for label in labels:\n",
    "\n",
    "            # Select the specific row from the negative_log_likelihood tensor\n",
    "            selected_row = negative_log_likelihood[idx]\n",
    "\n",
    "            # Determine the end index for slicing\n",
    "            e = s + label\n",
    "\n",
    "            # Slice the tensor from start_idx to end_idx\n",
    "            selected_elements = selected_row[s:e]\n",
    "\n",
    "            # Compute the mean of the selected elements\n",
    "            cur_loss = torch.mean(selected_elements)\n",
    "\n",
    "            loss.append(cur_loss)\n",
    "            s += 1\n",
    "\n",
    "    return torch.stack(loss).mean() / len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(val_dataloader, evi_dataloader, save_dir):\n",
    "    method_2 = False\n",
    "    sp = 0\n",
    "    if method_2:\n",
    "        \n",
    "        # # Define the total number of steps, 3000 is the train_dataloader length\n",
    "        total_steps = EPOCH * 3000\n",
    "        final_lr_steps = EPOCH/2 * 3000\n",
    "\n",
    "        # Define a lambda function to decrease the learning rate linearly over the first 5 epochs, and keep it constant afterwards\n",
    "        lr_lambda = lambda step: MAX_LR - (step / final_lr_steps) * (MAX_LR - 1e-5) if step < final_lr_steps else 1e-5\n",
    "\n",
    "        # Instantiate the scheduler\n",
    "        lmbda_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    # optional evaluation\n",
    "    print(\"\\nGenerating evidence embedding:\\n\")\n",
    "    evidence_embeddings, evidence_ids = embed_evidence(evi_dataloader, model)\n",
    "    print(\"\\nEvaluate evidence embedding, f-score:\\n\")\n",
    "    f_score = validate(val_dataloader, evidence_embeddings, evidence_ids, model)\n",
    "    # wandb.log({\"f_score\": f_score}, step=0)\n",
    "    print(f_score)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    \n",
    "    maximum_f_score = 0\n",
    "    patience = 5  # Number of epochs to wait for improvement before stopping\n",
    "    patience_counter = 0  # Counter to keep track of non-improving epochs\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCH):\n",
    "        sp += 1\n",
    "        \n",
    "        print(\"Generating training dataset with negative samples\")\n",
    "        train = TrainSet(evidence_embeddings, evidence_ids)\n",
    "        train_dataloader = DataLoader(train, BATCH_SIZE, shuffle=False, collate_fn=train.collate_fn)\n",
    "        del evidence_embeddings, evidence_ids\n",
    "\n",
    "        print(\"Starting epoch: \", epoch)\n",
    "        # Iterate over each batch in the data loader\n",
    "        for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "            # Move tensors to device\n",
    "            batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "\n",
    "            # Compute embeddings for claim and evidence\n",
    "            in_batch_claim_embeddings = extract_embeddings(batch, \"claim\")\n",
    "            in_batch_evidence_embeddings = extract_embeddings(batch, \"evidence\")\n",
    "\n",
    "            # Compute the loss and perform backpropagation\n",
    "            loss = compute_loss(in_batch_claim_embeddings, in_batch_evidence_embeddings, batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            del in_batch_claim_embeddings, in_batch_evidence_embeddings\n",
    "            \n",
    "        print(\"Finishing epoch: \", epoch)\n",
    "\n",
    "        # Evaluate the model every epoch\n",
    "        print(\"\\nGenerating evidence embedding:\")\n",
    "        evidence_embeddings, evidence_ids = embed_evidence(evi_dataloader, model)\n",
    "        print(\"\\nEvaluate evidence embedding, f-score:\")\n",
    "        f_score = validate(val_dataloader, evidence_embeddings, evidence_ids, model)\n",
    "        print(\"FSCORE\", f_score)\n",
    "        # wandb.log({\"f_score\": f_score}, step=epoch)\n",
    "\n",
    "        best_evidence_embeddings, best_evidence_ids = None, None\n",
    "        if f_score > maximum_f_score:\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, \"rtv_checkpoint.bin\"))\n",
    "            torch.save(evidence_embeddings, os.path.join(save_dir, \"evidence_embeddings\"))\n",
    "            torch.save(evidence_ids, os.path.join(save_dir, \"evidence_ids\"))\n",
    "\n",
    "            best_evidence_embeddings, best_evidence_ids = evidence_embeddings, evidence_ids\n",
    "            maximum_f_score = f_score\n",
    "            print(\"maximum_f_score\", f_score)\n",
    "            \n",
    "        else:\n",
    "\n",
    "            patience_counter += 1\n",
    "            print(\"No improvement in f_score, patience: \", patience_counter)\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered - no improvement in f_score for {} epochs\".format(patience))\n",
    "                break \n",
    "            \n",
    "    return best_evidence_embeddings, best_evidence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation dataloader\n",
    "validation = ValidationSet()\n",
    "val_dataloader = DataLoader(validation, batch_size=BATCH_SIZE, shuffle=False, collate_fn=validation.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evidence dataloader\n",
    "evidence = EvidenceSet()\n",
    "evi_dataloader = DataLoader(evidence, batch_size=BATCH_SIZE, shuffle=False, collate_fn=evidence.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "save_dir = makedir(\"rtv\")\n",
    "best_evidence_embeddings, best_evidence_ids = train(val_dataloader, evi_dataloader, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predication\n",
    "load_emb = False\n",
    "if load_emb:\n",
    "    best_evidence_embeddings = torch.load(\"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/rtv/model/rtv_13_05_2023/evidence_embeddings\")\n",
    "    best_evidence_ids = torch.load(\"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/rtv/model/rtv_13_05_2023/evidence_ids\")\n",
    "predict = False\n",
    "if predict:\n",
    "    predict(best_evidence_embeddings, best_evidence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
