{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "import json\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# wandb.init(project=\"nlp_project_cls\", name=\"bert base uncased test run\")\n",
    "\n",
    "# PATHS\n",
    "DEV_CLAIMS_BASELINE_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/dev-claims-baseline.json\"\n",
    "DEV_CLAIMS_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/dev-claims.json\"\n",
    "EVIDENCE_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/evidence.json\"\n",
    "SMALL_EVIDENCE_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/small_evidence.json\"\n",
    "TINY_EVIDENCE_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/tiny_evidence.json\"\n",
    "CODE_DEV_EVIDENCE_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/code_dev_evidence.json\"\n",
    "TEST_CLAIMS_UNLABELLED_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/test-claims-unlabelled.json\"\n",
    "TEST_CLAIMS_LABELLED_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/test-claims-labelled.json\"\n",
    "TRAIN_CLAIMS_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/train-claims.json\"\n",
    "RETRIEVAL_TEST_CLAIMS_JSON_PATH = \"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/data/retrieval-test-claims.json\"\n",
    "\n",
    "# ARGS\n",
    "BATCH_SIZE = 4\n",
    "EPOCH = 1\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LR = 2e-5\n",
    "MAX_LENGTH = 256\n",
    "RETRIEVAL_NUM = 3\n",
    "K = 4\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"using mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"using cpu\")\n",
    "\n",
    "\n",
    "LABEL_TO_INT = {\n",
    "                'NOT_ENOUGH_INFO': 0,\n",
    "                'DISPUTED': 1,\n",
    "                'REFUTES': 2,\n",
    "                'SUPPORTS': 3,\n",
    "                    }\n",
    "\n",
    "INT_TO_LABEL = {\n",
    "                0: 'NOT_ENOUGH_INFO',\n",
    "                1: 'DISPUTED',\n",
    "                2: 'REFUTES',\n",
    "                3: 'SUPPORTS',\n",
    "                }\n",
    "\n",
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, pre_encoder):\n",
    "\n",
    "        super(NNClassifier, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(pre_encoder)\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_ids=batch[\"batched_input_ids\"]\n",
    "        attention_mask=batch[\"batched_attention_mask\"]\n",
    "        texts_emb = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        texts_emb = texts_emb[:, 0, :]\n",
    "        res = self.cls(texts_emb)\n",
    "\n",
    "        return res\n",
    "\n",
    "# bert model\n",
    "model = NNClassifier(MODEL_NAME)\n",
    "model = model.to(device)  # Move model to device\n",
    "loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Instantiate the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=MAX_LR, weight_decay=1e-4)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Instantiate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "load = True\n",
    "if load:\n",
    "    model.load_state_dict(torch.load(os.path.join(\"/Users/taylortang/Life-at-UniMelb/Semester_3/COMP90042_NLP/Project_2/code/rtv/model/cls_15_05_2023/cls_checkpoint.bin\")))\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def process(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    cleaned = ''.join([char if char.isalnum() or char.isspace() else '' for char in sentence])\n",
    "    return cleaned\n",
    "\n",
    "# small util\n",
    "def tokenization(text):\n",
    "    tokens = tokenizer(text, max_length=MAX_LENGTH, padding=True, return_tensors=\"pt\", truncation=True)\n",
    "    return tokens\n",
    "\n",
    "# small util\n",
    "def makedir(sub_dir):\n",
    "    date = datetime.now().strftime(\"%d_%m_%Y\")\n",
    "    save_dir = f\"./model/{sub_dir}_{date}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    return save_dir\n",
    "\n",
    "def concat_texts(claim, evidences):\n",
    "    claim_text = [process(claim[\"claim_text\"])]\n",
    "    if \"evidences\" in claim.keys():\n",
    "        for evidence_id in claim[\"evidences\"]:\n",
    "            claim_text.append(process(evidences[evidence_id]))\n",
    "    return claim_text\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_encoding = dict()\n",
    "    claim_texts, labels, claims, claim_ids = zip(*batch)\n",
    "    tokens = tokenization(claim_texts)\n",
    "    batch_encoding[\"batched_input_ids\"] = tokens.input_ids\n",
    "    batch_encoding[\"batched_attention_mask\"] = tokens.attention_mask\n",
    "    batch_encoding[\"claims\"] = claims\n",
    "    batch_encoding[\"claim_ids\"] = claim_ids\n",
    "    batch_encoding[\"label\"] = labels\n",
    "\n",
    "    return batch_encoding\n",
    "\n",
    "class ClassificationTrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        with open(TRAIN_CLAIMS_JSON_PATH, \"r\") as file:\n",
    "            self.data = json.load(file)\n",
    "\n",
    "        with open(TRAIN_CLAIMS_JSON_PATH, \"r\") as file:\n",
    "            self.evidences = json.load(file)\n",
    "\n",
    "        self.claim_ids = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_ids)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        claim_id = self.all_claim_keys[id]\n",
    "        claim = self.data.get(claim_id)\n",
    "        claim_text = concat_texts(claim, self.evidences)\n",
    "        label = LABEL_TO_INT[claim[\"claim_label\"]]\n",
    "\n",
    "        return [claim_text, label, claim, self.claim_ids[id]]\n",
    "    \n",
    "class ClassificationValidationDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        with open(DEV_CLAIMS_JSON_PATH, \"r\") as file:\n",
    "            self.data = json.load(file)\n",
    "\n",
    "        with open(TRAIN_CLAIMS_JSON_PATH, \"r\") as file:\n",
    "            self.evidences = json.load(file)\n",
    "\n",
    "        self.claim_ids = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_ids)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        claim_id = self.all_claim_keys[id]\n",
    "        claim = self.data.get(claim_id)\n",
    "        claim_text = concat_texts(claim, self.evidences)\n",
    "        label = LABEL_TO_INT[claim[\"claim_label\"]]\n",
    "\n",
    "        return [claim_text, label, claim, self.claim_ids[id]]\n",
    "    \n",
    "class ClassificationTestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        with open(TEST_CLAIMS_UNLABELLED_JSON_PATH, \"r\") as file:\n",
    "            self.data = json.load(file)\n",
    "\n",
    "        with open(EVIDENCE_JSON_PATH, \"r\") as file:\n",
    "            self.evidences = json.load(file)\n",
    "\n",
    "        self.claim_ids = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_ids)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        claim_id = self.all_claim_keys[id]\n",
    "        claim = self.data.get(claim_id)\n",
    "        claim_text = [process(claim[\"claim_text\"])]\n",
    "        claim_text = concat_texts(claim, self.evidences)\n",
    "        label = LABEL_TO_INT[claim[\"claim_label\"]]\n",
    "\n",
    "        return [claim_text, label, claim, self.claim_ids[id]]\n",
    "\n",
    "def predict():\n",
    "    test_set = ClassificationTestDataset()\n",
    "    dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    model.eval()\n",
    "\n",
    "    prediction = {}\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "            output = model(batch)\n",
    "            predicted_labels = output.argmax(-1)\n",
    "            predicted_labels = predicted_labels.tolist()\n",
    "\n",
    "            for index, (data, predicted_label) in enumerate(zip(batch[\"datas\"], predicted_labels)):\n",
    "                string_label = INT_TO_LABEL[predicted_label]\n",
    "                data[\"claim_label\"] = string_label\n",
    "                claim_id = batch[\"claim_ids\"][index]\n",
    "                prediction[claim_id] = data\n",
    "\n",
    "        with open(TEST_CLAIMS_UNLABELLED_JSON_PATH, 'w') as file:\n",
    "            json.dump(prediction, file)\n",
    "    \n",
    "def validate(dataloader, model):\n",
    "    model.eval()  # switch model to the evaluation mode\n",
    "\n",
    "    total_examples = 0.0\n",
    "    total_correct = 0.0\n",
    "\n",
    "    for data_batch in tqdm(dataloader):\n",
    "        data_batch = {key: value.to(device) if torch.is_tensor(value) else value for key, value in data_batch.items()}\n",
    "\n",
    "        outputs = model(data_batch)\n",
    "        predicted_labels = outputs.argmax(-1)\n",
    "\n",
    "        # Check for each label if the prediction matches the true label\n",
    "        is_prediction_correct = []\n",
    "        for i in range(len(predicted_labels)):\n",
    "            if predicted_labels[i] == data_batch[\"label\"][i]:\n",
    "                is_prediction_correct.append(True)\n",
    "            else:\n",
    "                is_prediction_correct.append(False)\n",
    "\n",
    "        # Sum up the number of correct predictions\n",
    "        sum_of_correct_predictions = sum(is_prediction_correct)  # Corrected here\n",
    "\n",
    "        total_correct += sum_of_correct_predictions\n",
    "        total_examples += predicted_labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_examples\n",
    "    model.train()  # switch model back to training mode\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def train(train_dataloader, val_dataloader, save_dir):\n",
    "    model.train()\n",
    "    maximum_accuracy = 0\n",
    "\n",
    "    for i in range(EPOCH):\n",
    "        for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "\n",
    "            res = model(batch)\n",
    "            loss = loss_function(res, batch[\"label\"])\n",
    "            loss = loss / BATCH_SIZE\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # wandb.log({\"accuracy\": acc}, step=all_step_cnt)\n",
    "\n",
    "        accuracy = validate(val_dataloader, model)\n",
    "        if accuracy > maximum_accuracy:\n",
    "            maximum_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, \"cls_checkpoint.bin\"))\n",
    "            print(\"mmaximum_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = makedir(\"cls\")\n",
    "\n",
    "train_set = ClassificationTrainDataset()\n",
    "val_set = ClassificationValidationDataset()\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "train(train_dataloader, val_dataloader, save_dir)\n",
    "predict = False\n",
    "if predict:\n",
    "    predict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
